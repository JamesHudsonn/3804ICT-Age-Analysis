{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Birth year</th>\n",
       "      <th>Death year</th>\n",
       "      <th>Manner of death</th>\n",
       "      <th>Age of death</th>\n",
       "      <th>Associated Countries</th>\n",
       "      <th>Associated Country Life Expectancy</th>\n",
       "      <th>AOD_Category</th>\n",
       "      <th>Gender Grouped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>douglas adams</td>\n",
       "      <td>['male']</td>\n",
       "      <td>artist</td>\n",
       "      <td>1952</td>\n",
       "      <td>2001</td>\n",
       "      <td>natural causes</td>\n",
       "      <td>49</td>\n",
       "      <td>['united kingdom']</td>\n",
       "      <td>[81.3]</td>\n",
       "      <td>41-50</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abraham lincoln</td>\n",
       "      <td>['male']</td>\n",
       "      <td>politician</td>\n",
       "      <td>1809</td>\n",
       "      <td>1865</td>\n",
       "      <td>homicide</td>\n",
       "      <td>56</td>\n",
       "      <td>['united states']</td>\n",
       "      <td>[78.5]</td>\n",
       "      <td>51-60</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paul morand</td>\n",
       "      <td>['male']</td>\n",
       "      <td>artist</td>\n",
       "      <td>1888</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>['france']</td>\n",
       "      <td>[82.5]</td>\n",
       "      <td>81-90</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude monet</td>\n",
       "      <td>['male']</td>\n",
       "      <td>artist</td>\n",
       "      <td>1840</td>\n",
       "      <td>1926</td>\n",
       "      <td>natural causes</td>\n",
       "      <td>86</td>\n",
       "      <td>['france']</td>\n",
       "      <td>[82.5]</td>\n",
       "      <td>81-90</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elvis presley</td>\n",
       "      <td>['male']</td>\n",
       "      <td>artist</td>\n",
       "      <td>1935</td>\n",
       "      <td>1977</td>\n",
       "      <td>natural causes</td>\n",
       "      <td>42</td>\n",
       "      <td>['united states']</td>\n",
       "      <td>[78.5]</td>\n",
       "      <td>41-50</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name    Gender  Occupation  Birth year  Death year  \\\n",
       "0    douglas adams  ['male']      artist        1952        2001   \n",
       "1  abraham lincoln  ['male']  politician        1809        1865   \n",
       "2      paul morand  ['male']      artist        1888        1976   \n",
       "3     claude monet  ['male']      artist        1840        1926   \n",
       "4    elvis presley  ['male']      artist        1935        1977   \n",
       "\n",
       "  Manner of death  Age of death Associated Countries  \\\n",
       "0  natural causes            49   ['united kingdom']   \n",
       "1        homicide            56    ['united states']   \n",
       "2             NaN            88           ['france']   \n",
       "3  natural causes            86           ['france']   \n",
       "4  natural causes            42    ['united states']   \n",
       "\n",
       "  Associated Country Life Expectancy AOD_Category Gender Grouped  \n",
       "0                             [81.3]        41-50           male  \n",
       "1                             [78.5]        51-60           male  \n",
       "2                             [82.5]        81-90           male  \n",
       "3                             [82.5]        81-90           male  \n",
       "4                             [78.5]        41-50           male  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_ages_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 742532 entries, 0 to 742531\n",
      "Data columns (total 11 columns):\n",
      " #   Column                              Non-Null Count   Dtype \n",
      "---  ------                              --------------   ----- \n",
      " 0   Name                                742532 non-null  object\n",
      " 1   Gender                              666992 non-null  object\n",
      " 2   Occupation                          684916 non-null  object\n",
      " 3   Birth year                          742532 non-null  int64 \n",
      " 4   Death year                          742532 non-null  int64 \n",
      " 5   Manner of death                     43779 non-null   object\n",
      " 6   Age of death                        742532 non-null  int64 \n",
      " 7   Associated Countries                742532 non-null  object\n",
      " 8   Associated Country Life Expectancy  742304 non-null  object\n",
      " 9   AOD_Category                        742532 non-null  object\n",
      " 10  Gender Grouped                      742532 non-null  object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 62.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The data must be converted into a format that can be used for the Apriori algorithm. This means converting each item in the dataset into a 'transaction'. Each transaction is a set of attribute values, we use six columns for this: 'Gender', 'Occupation', 'Manner of death', 'Age of death', 'AOD_Category', and 'Associated Countries'.\n",
    "\n",
    "an example of a transaction:<br>\n",
    "{’AOD_Category=41-50’,’Age of death=49’,’Associated Countries=united kingdom’,’Gender=male’,’Manner of death=natural causes’,’Occupation=artist’}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'AOD_Category=41-50',\n",
       "  'Age of death=49',\n",
       "  'Associated Countries=united kingdom',\n",
       "  'Gender=male',\n",
       "  'Occupation=artist'},\n",
       " {'AOD_Category=51-60',\n",
       "  'Age of death=56',\n",
       "  'Associated Countries=united states',\n",
       "  'Gender=male',\n",
       "  'Occupation=politician'},\n",
       " {'AOD_Category=81-90',\n",
       "  'Age of death=88',\n",
       "  'Associated Countries=france',\n",
       "  'Gender=male',\n",
       "  'Occupation=artist'},\n",
       " {'AOD_Category=81-90',\n",
       "  'Age of death=86',\n",
       "  'Associated Countries=france',\n",
       "  'Gender=male',\n",
       "  'Occupation=artist'},\n",
       " {'AOD_Category=41-50',\n",
       "  'Age of death=42',\n",
       "  'Associated Countries=united states',\n",
       "  'Gender=male',\n",
       "  'Occupation=artist'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with null values in 'Associated Countries' and 'Gender' columns\n",
    "df_with_mod = df.dropna(subset=['Associated Countries', 'Gender'])\n",
    "df_without_mod = df_with_mod.copy()\n",
    "df_without_mod.drop('Manner of death', axis=1, inplace=True)\n",
    "selected_columns = ['Gender', 'Occupation', 'Age of death', 'AOD_Category', 'Associated Countries']\n",
    "\n",
    "# Initialize an empty list to hold the transactions\n",
    "transactions = []\n",
    "\n",
    "# Convert each row in the dataset to a transaction\n",
    "for index, row in df_without_mod.iterrows():\n",
    "    transaction = []\n",
    "    for col in selected_columns:\n",
    "        # Special handling for 'Associated Countries' and 'Gender' columns which may have multiple values\n",
    "        if col == 'Associated Countries' or col == 'Gender':\n",
    "            # Convert the string representation to a list\n",
    "            values = ast.literal_eval(row[col]) if isinstance(row[col], str) else row[col]\n",
    "            for value in values:\n",
    "                item = f\"{col}={value}\"\n",
    "                transaction.append(item)\n",
    "        else:\n",
    "            # Create an item by combining the column name and the value (e.g., \"Gender=male\")\n",
    "            item = f\"{col}={row[col]}\"\n",
    "            transaction.append(item)\n",
    "    transactions.append(set(transaction))\n",
    "\n",
    "# Show the first 5 final revised transactions\n",
    "transactions[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'AOD_Category=71-80',\n",
       "  'Age of death=71',\n",
       "  'Associated Countries=germany',\n",
       "  'Associated Countries=israel',\n",
       "  'Gender=intersex',\n",
       "  'Gender=transgender male',\n",
       "  'Gender=transgender person',\n",
       "  'Occupation=artist'},\n",
       " {'AOD_Category=61-70',\n",
       "  'Age of death=65',\n",
       "  'Associated Countries=united states',\n",
       "  'Gender=female',\n",
       "  'Gender=transgender female',\n",
       "  'Occupation=artist'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code to print out transactions with multiple 'Associated Countries' and 'Gender'\n",
    "multigendertransactions = [transaction for transaction in transactions \n",
    "                            if  len([item for item in transaction if 'Gender=' in item]) > 1]\n",
    "\n",
    "multigendertransactions[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gender=male', 594457),\n",
       " ('Occupation=artist', 180230),\n",
       " ('AOD_Category=71-80', 179211),\n",
       " ('AOD_Category=81-90', 161078),\n",
       " ('Associated Countries=united states', 147009),\n",
       " ('AOD_Category=61-70', 128614),\n",
       " ('Occupation=politician', 121811),\n",
       " ('Occupation=athlete', 89928),\n",
       " ('Associated Countries=germany', 77554),\n",
       " ('Associated Countries=united kingdom', 75757)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate the frequency of each item across all transactions\n",
    "item_frequency = defaultdict(int)\n",
    "\n",
    "# Iterate through each transaction and count the occurrence of each item\n",
    "for transaction in transactions:\n",
    "    for item in transaction:\n",
    "        item_frequency[item] += 1\n",
    "\n",
    "# Sort items by frequency\n",
    "sorted_items = sorted(item_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Show some of the most frequent items\n",
    "sorted_items[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Candidate Itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({'Occupation=politician'}),\n",
       " frozenset({'Occupation=researcher'}),\n",
       " frozenset({'Associated Countries=germany'}),\n",
       " frozenset({'Gender=female'}),\n",
       " frozenset({'AOD_Category=61-70'}),\n",
       " frozenset({'AOD_Category=81-90'}),\n",
       " frozenset({'Associated Countries=austria'}),\n",
       " frozenset({'AOD_Category=71-80'}),\n",
       " frozenset({'Associated Countries=united states'}),\n",
       " frozenset({'AOD_Category=51-60'})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_support_no_optimisations(itemset, transactions):\n",
    "    itemset_count = 0\n",
    "    total_transactions = len(transactions)\n",
    "    for transaction in transactions:\n",
    "        if itemset.issubset(transaction):\n",
    "            itemset_count += 1\n",
    "    support = itemset_count / total_transactions\n",
    "    return support\n",
    "\n",
    "\n",
    "def generate_candidates__no_optimisations(prev_frequent_itemsets):\n",
    "    new_candidates = set()\n",
    "    for i in prev_frequent_itemsets:\n",
    "        for j in prev_frequent_itemsets:\n",
    "            union_set = i.union(j)\n",
    "            if len(union_set) == len(i) + 1:\n",
    "                new_candidates.add(union_set)\n",
    "    return new_candidates\n",
    "\n",
    "\n",
    "def apriori_no_optimisations(transactions, min_support=0.1):\n",
    "    frequent_itemsets = []\n",
    "    \n",
    "    # Generate initial candidates (1-itemsets)\n",
    "    initial_candidates = set()\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            initial_candidates.add(frozenset([item]))\n",
    "            \n",
    "    # Find frequent 1-itemsets\n",
    "    frequent_1_itemsets = set()\n",
    "    for itemset in initial_candidates:\n",
    "        if calculate_support_no_optimisations(itemset, transactions) >= min_support:\n",
    "            frequent_1_itemsets.add(itemset)\n",
    "    frequent_itemsets.extend(frequent_1_itemsets)\n",
    "    \n",
    "    prev_frequent_itemsets = frequent_1_itemsets\n",
    "    \n",
    "    # Generate new candidates without pruning\n",
    "    while len(prev_frequent_itemsets) > 0:\n",
    "        new_candidates = generate_candidates__no_optimisations(prev_frequent_itemsets)\n",
    "        new_frequent_itemsets = set()\n",
    "        for itemset in new_candidates:\n",
    "            if calculate_support_no_optimisations(itemset, transactions) >= min_support:\n",
    "                new_frequent_itemsets.add(itemset)\n",
    "        frequent_itemsets.extend(new_frequent_itemsets)\n",
    "        prev_frequent_itemsets = new_frequent_itemsets\n",
    "        \n",
    "    return frequent_itemsets\n",
    "\n",
    "min_support = 0.1\n",
    "min_hash_count = 2  # Minimum count for hash table\n",
    "subset_transactions = transactions[:10000]  # Adjust the subset size as needed\n",
    "frequent_itemsets = apriori_no_optimisations(subset_transactions, min_support)\n",
    "frequent_itemsets[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations to the Apriori Algorithm\n",
    "\n",
    "The original Apriori was slow for big datasets. I implemented these four improvements:\n",
    "\n",
    "1. **Early Pruning**: I cut down the list of possible item sets early on. This means fewer item sets to check later.\n",
    "   \n",
    "2. **Transaction Reduction**: I removed data rows that were already fully explained by frequent item sets. This makes the data smaller and faster to scan.\n",
    "\n",
    "3. **Hash Method**: I used a hash table to keep track of item sets. This is faster than using lists or sets.\n",
    "\n",
    "4. **Twice-Only Partitioning**: I split the data into smaller parts and ran Apriori on each. The results were then used for a final check on the full data. This means fewer item sets to check in the end.\n",
    "\n",
    "These changes did not make any noticeable difference to the efficiency of the algorithm. This was baffling until I discovered  using the library apriori functions did not yield any association rules.\n",
    "\n",
    "#### Why Optimisations Dont Help\n",
    "\n",
    "**Low Support**: If the support for all multi-item itemsets in the dataset is below the minimum threshold the algorithm will spend a lot of time generating and checking itemsets that end up being discarded, which will render most optomisations ineffective.\n",
    "\n",
    "**Sparse Data**: If the data is sparse, meaning there are few recurring patterns of items appearing together, then most of the candidate itemsets would be infrequent, leading to a large number of checks but not a lot of pruning or transaction reductions.\n",
    "\n",
    "**Lack of Redundant Transactions**: If the dataset doesn't have many redundant transactions, then the transaction reduction step would not help much.\n",
    "\n",
    "**Ineffective Pruning**: If the data is such that very few itemsets can be pruned early, the pruning step becomes almost useless, leading to a full scan of the dataset for each candidate set.\n",
    "\n",
    "**Highly Varied Data**: If the data has high variance in terms of the items in transactions, then the hash-based technique and transaction reduction would not provide much benefit.\n",
    "\n",
    "**Lack of Strong Associations**: Finally, the absence of association rules suggests that the data doesn't have strong associations between items. In this case, the algorithm has to go through the complete search space without finding anything substantial, making optimizations less effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({'Gender=male'})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple hash function using ascii values of the first letter of each item\n",
    "def simple_hash(itemset):\n",
    "    return sum(ord(item[0]) for item in itemset)\n",
    "\n",
    "def calculate_support(itemset, transactions):\n",
    "    itemset_count = 0\n",
    "    total_transactions = len(transactions)\n",
    "    for transaction in transactions:\n",
    "        if itemset.issubset(transaction):\n",
    "            itemset_count += 1\n",
    "    support = itemset_count / total_transactions\n",
    "    return support\n",
    "\n",
    "\n",
    "def generate_and_prune_candidates(prev_frequent_itemsets, min_hash_count=2):\n",
    "    new_candidates = set()\n",
    "    hash_table = {}\n",
    "    \n",
    "    # Generate candidate itemsets\n",
    "    for i in prev_frequent_itemsets:\n",
    "        for j in prev_frequent_itemsets:\n",
    "            union_set = i.union(j)\n",
    "            if len(union_set) == len(i) + 1:\n",
    "                new_candidates.add(union_set)\n",
    "                \n",
    "                # Apply hash function\n",
    "                hash_value = simple_hash(union_set)\n",
    "                \n",
    "                # Update hash table\n",
    "                if hash_value in hash_table:\n",
    "                    hash_table[hash_value] += 1\n",
    "                else:\n",
    "                    hash_table[hash_value] = 1\n",
    "    \n",
    "    # Prune candidates using hash table\n",
    "    pruned_candidates = set()\n",
    "    for itemset in new_candidates:\n",
    "        hash_value = simple_hash(itemset)\n",
    "        if hash_table.get(hash_value, 0) >= min_hash_count:\n",
    "            pruned_candidates.add(itemset)\n",
    "            \n",
    "    return pruned_candidates\n",
    "\n",
    "def remove_redundant_transactions(transactions, frequent_itemsets):\n",
    "    # Initialize an empty list to hold the new transactions after removal\n",
    "    new_transactions = []\n",
    "    \n",
    "    # Loop through each transaction in the list of transactions\n",
    "    for transaction in transactions:\n",
    "        is_redundant = True  # Assume the transaction is redundant until proven otherwise\n",
    "        \n",
    "        # Loop through each item in the transaction\n",
    "        for item in transaction:\n",
    "            # Check if this item is part of any frequent itemset\n",
    "            is_item_in_frequent_itemset = False\n",
    "            for frequent_itemset in frequent_itemsets:\n",
    "                if frozenset([item]).issubset(frequent_itemset):\n",
    "                    is_item_in_frequent_itemset = True\n",
    "                    break\n",
    "            \n",
    "            # If the item is not in any frequent itemset, then the transaction is not redundant\n",
    "            if not is_item_in_frequent_itemset:\n",
    "                is_redundant = False\n",
    "                break\n",
    "        \n",
    "        # If the transaction is not redundant, add it to the new list of transactions\n",
    "        if not is_redundant:\n",
    "            new_transactions.append(transaction)\n",
    "            \n",
    "    return new_transactions\n",
    "\n",
    "\n",
    "def apriori_with_hashing(transactions, min_support=0.1, min_hash_count=2):\n",
    "    frequent_itemsets = []\n",
    "    \n",
    "    # Generate initial candidates (1-itemsets)\n",
    "    initial_candidates = set()\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            initial_candidates.add(frozenset([item]))\n",
    "            \n",
    "    # Find frequent 1-itemsets\n",
    "    frequent_1_itemsets = set()\n",
    "    for itemset in initial_candidates:\n",
    "        if calculate_support(itemset, transactions) >= min_support:\n",
    "            frequent_1_itemsets.add(itemset)\n",
    "    frequent_itemsets.extend(frequent_1_itemsets)\n",
    "    \n",
    "    prev_frequent_itemsets = frequent_1_itemsets\n",
    "    \n",
    "    # Generate and prune new candidates\n",
    "    while len(prev_frequent_itemsets) > 0:\n",
    "        # Remove redundant transactions\n",
    "        transactions = remove_redundant_transactions(transactions, prev_frequent_itemsets)\n",
    "        # Generate and prune candidates using hash-based technique\n",
    "        new_candidates = generate_and_prune_candidates(prev_frequent_itemsets, min_hash_count)\n",
    "        new_frequent_itemsets = set()\n",
    "        for itemset in new_candidates:\n",
    "            if calculate_support(itemset, transactions) >= min_support:\n",
    "                new_frequent_itemsets.add(itemset)\n",
    "        frequent_itemsets.extend(new_frequent_itemsets)\n",
    "        prev_frequent_itemsets = new_frequent_itemsets\n",
    "        \n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "def apriori_with_twice_only_partitioning(transactions, min_support=0.1):\n",
    "    # Automatically calculate the partition size to evenly divide the dataset\n",
    "    num_partitions = 10  # You can change this number as needed\n",
    "    partition_size = len(transactions) // num_partitions\n",
    "    \n",
    "    # Create partitions\n",
    "    partitions = [transactions[i:i + partition_size] for i in range(0, len(transactions), partition_size)]\n",
    "    \n",
    "    local_frequent_itemsets = set()\n",
    "    for partition in partitions:\n",
    "        local_frequent = apriori_with_hashing(partition, min_support)\n",
    "        local_frequent_itemsets.update(local_frequent)\n",
    "    \n",
    "    global_frequent_itemsets = set()\n",
    "    for itemset in local_frequent_itemsets:\n",
    "        if calculate_support(itemset, transactions) >= min_support:\n",
    "            global_frequent_itemsets.add(itemset)\n",
    "            \n",
    "    return global_frequent_itemsets\n",
    "# Test the Apriori algorithm with Hash-based technique, using a subset of transactions and a minimum support of 0.1\n",
    "# Using a subset for testing purposes due to computational intensity\n",
    "min_support_with_hashing = 0.5\n",
    "min_hash_count = 2  # Minimum count for hash table\n",
    "subset_transactions_with_hashing = transactions[:100000]  # Adjust the subset size as needed\n",
    "frequent_itemsets_with_hashing = apriori_with_hashing(subset_transactions_with_hashing, min_support_with_hashing, min_hash_count)\n",
    "\n",
    "# Show some frequent itemsets\n",
    "frequent_itemsets_with_hashing[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

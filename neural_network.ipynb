{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Notebook\n",
    "Main questions being answered:\n",
    "1. Can the age of death based on features like gender, occupation, birth year, and associated country be predicted?\n",
    "2. Can the nn accurately classify the manner of death (e.g., natural causes, accidents) based on the available features?\n",
    "3. Can any non-linear relationships be discerned between occupation type and age of death or manner of death?\n",
    "4. Can any outlier cases be identified / predicted, such as individuals who lived significantly longer or shorter than the average life expectancy of their associated country?\n",
    "5. Can the life expectancy of a country based on the aggregate data of individuals associated with that country be predicted?\n",
    "6. How well does the neural network generalize its predictions to countries or occupations that are underrepresented in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant packages\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the preprocessed age data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('preprocessed_age.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Implementation #1  \n",
    "Code for the Neural Network using Keras and Tensorflow. It is a simple feedforward network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neural_network(data, features, target, epochs=100, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Define, compile, train, and evaluate a neural network model.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The dataset.\n",
    "        features (list): List of feature column names.\n",
    "        target (str): Target column name.\n",
    "        epochs (int): Number of training epochs. Default is 100.\n",
    "        test_size (float): Proportion of data to be used for testing. Default is 0.2.\n",
    "        random_state (int): Random state for train-test split. Default is 42.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained neural network model.\n",
    "        history: Training history of the model.\n",
    "    \"\"\"\n",
    "    # Data Preparation\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), X.select_dtypes(include=['int64', 'float64']).columns.tolist()),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), X.select_dtypes(include=['object']).columns.tolist())\n",
    "        ])\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert the preprocessed data to dense format and then to tf.data.Dataset format\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_preprocessed.todense(), y_train)).batch(32)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test_preprocessed.todense(), y_test)).batch(32)\n",
    "\n",
    "    \n",
    "    # Define the model\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(1633,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Check input data shape\n",
    "    print(f'Training data shape: {X_train.shape}')\n",
    "    print(f'Test data shape: {X_test.shape}')\n",
    "\n",
    "    # Check for NaN or Inf values\n",
    "    print(f'NaN values in training data: {X_train.isna().sum().sum()}')\n",
    "    print(f'NaN values in test data: {X_test.isna().sum().sum()}')\n",
    "    print(f'Inf values in training data: {(X_train == np.inf).sum().sum()}')\n",
    "    print(f'Inf values in test data: {(X_test == np.inf).sum().sum()}')\n",
    "\n",
    "    # Display model architecture\n",
    "    model.summary()\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model using tf.data.Dataset format\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, mae = model.evaluate(test_dataset)\n",
    "    print(f'Mean Absolute Error on test data: {mae}')\n",
    "    \n",
    "    # Visualize the training process\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Implementation #2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can the age of death based on features like gender, occupation, birth year, and associated country be predicted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using implmentation #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (796829, 4)\n",
      "Test data shape: (199208, 4)\n",
      "NaN values in training data: 202485\n",
      "NaN values in test data: 51020\n",
      "Inf values in training data: 0\n",
      "Inf values in test data: 0\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                104576    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 106689 (416.75 KB)\n",
      "Trainable params: 106689 (416.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "24901/24901 [==============================] - 62s 2ms/step - loss: 260.0850 - mae: 12.1385 - val_loss: 223.7039 - val_mae: 11.3735\n",
      "Epoch 2/100\n",
      "24901/24901 [==============================] - 58s 2ms/step - loss: 222.7914 - mae: 11.5013 - val_loss: 223.1134 - val_mae: 11.3658\n",
      "Epoch 3/100\n",
      "24901/24901 [==============================] - 57s 2ms/step - loss: 222.2359 - mae: 11.4846 - val_loss: 222.8177 - val_mae: 11.3668\n",
      "Epoch 4/100\n",
      "24901/24901 [==============================] - 56s 2ms/step - loss: 221.8868 - mae: 11.4739 - val_loss: 222.6356 - val_mae: 11.3657\n",
      "Epoch 5/100\n",
      "24901/24901 [==============================] - 56s 2ms/step - loss: 221.6115 - mae: 11.4656 - val_loss: 222.4340 - val_mae: 11.3738\n",
      "Epoch 6/100\n",
      "24901/24901 [==============================] - 56s 2ms/step - loss: 221.3809 - mae: 11.4584 - val_loss: 222.3439 - val_mae: 11.3784\n",
      "Epoch 7/100\n",
      "24901/24901 [==============================] - 53s 2ms/step - loss: 221.1914 - mae: 11.4526 - val_loss: 222.1784 - val_mae: 11.3859\n",
      "Epoch 8/100\n",
      "24901/24901 [==============================] - 53s 2ms/step - loss: 221.0310 - mae: 11.4479 - val_loss: 222.1233 - val_mae: 11.3906\n",
      "Epoch 9/100\n",
      "24901/24901 [==============================] - 57s 2ms/step - loss: 220.8954 - mae: 11.4439 - val_loss: 222.1122 - val_mae: 11.3958\n",
      "Epoch 10/100\n",
      "24901/24901 [==============================] - 55s 2ms/step - loss: 220.7829 - mae: 11.4404 - val_loss: 222.0633 - val_mae: 11.3981\n",
      "Epoch 11/100\n",
      "24901/24901 [==============================] - 60s 2ms/step - loss: 220.6781 - mae: 11.4372 - val_loss: 222.0373 - val_mae: 11.4008\n",
      "Epoch 12/100\n",
      "24901/24901 [==============================] - 61s 2ms/step - loss: 220.5809 - mae: 11.4343 - val_loss: 222.0326 - val_mae: 11.4049\n",
      "Epoch 13/100\n",
      "24901/24901 [==============================] - 58s 2ms/step - loss: 220.4880 - mae: 11.4313 - val_loss: 221.9787 - val_mae: 11.4060\n",
      "Epoch 14/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 220.4016 - mae: 11.4285 - val_loss: 221.9270 - val_mae: 11.4105\n",
      "Epoch 15/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 220.3224 - mae: 11.4260 - val_loss: 221.8954 - val_mae: 11.4156\n",
      "Epoch 16/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 220.2631 - mae: 11.4239 - val_loss: 221.9005 - val_mae: 11.4158\n",
      "Epoch 17/100\n",
      "24901/24901 [==============================] - 57s 2ms/step - loss: 220.1969 - mae: 11.4216 - val_loss: 221.9049 - val_mae: 11.4153\n",
      "Epoch 18/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 220.1423 - mae: 11.4199 - val_loss: 221.9270 - val_mae: 11.4170\n",
      "Epoch 19/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 220.0913 - mae: 11.4183 - val_loss: 221.9083 - val_mae: 11.4192\n",
      "Epoch 20/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 220.0359 - mae: 11.4163 - val_loss: 221.8984 - val_mae: 11.4243\n",
      "Epoch 21/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 219.9835 - mae: 11.4148 - val_loss: 221.8984 - val_mae: 11.4290\n",
      "Epoch 22/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 219.9162 - mae: 11.4125 - val_loss: 221.9303 - val_mae: 11.4332\n",
      "Epoch 23/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 219.8685 - mae: 11.4109 - val_loss: 221.9371 - val_mae: 11.4344\n",
      "Epoch 24/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.8279 - mae: 11.4096 - val_loss: 221.9321 - val_mae: 11.4376\n",
      "Epoch 25/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.7852 - mae: 11.4081 - val_loss: 221.9465 - val_mae: 11.4392\n",
      "Epoch 26/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.7454 - mae: 11.4068 - val_loss: 221.9328 - val_mae: 11.4433\n",
      "Epoch 27/100\n",
      "24901/24901 [==============================] - 53s 2ms/step - loss: 219.7047 - mae: 11.4056 - val_loss: 221.9443 - val_mae: 11.4460\n",
      "Epoch 28/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.6684 - mae: 11.4042 - val_loss: 221.9668 - val_mae: 11.4496\n",
      "Epoch 29/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 219.6340 - mae: 11.4032 - val_loss: 221.9913 - val_mae: 11.4510\n",
      "Epoch 30/100\n",
      "24901/24901 [==============================] - 59s 2ms/step - loss: 219.6113 - mae: 11.4020 - val_loss: 222.0364 - val_mae: 11.4546\n",
      "Epoch 31/100\n",
      "24901/24901 [==============================] - 57s 2ms/step - loss: 219.5734 - mae: 11.4010 - val_loss: 222.0655 - val_mae: 11.4547\n",
      "Epoch 32/100\n",
      "24901/24901 [==============================] - 56s 2ms/step - loss: 219.5430 - mae: 11.3998 - val_loss: 222.0989 - val_mae: 11.4567\n",
      "Epoch 33/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.5163 - mae: 11.3991 - val_loss: 222.1111 - val_mae: 11.4554\n",
      "Epoch 34/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.4893 - mae: 11.3981 - val_loss: 222.1071 - val_mae: 11.4577\n",
      "Epoch 35/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.4620 - mae: 11.3973 - val_loss: 222.1302 - val_mae: 11.4573\n",
      "Epoch 36/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.4330 - mae: 11.3964 - val_loss: 222.1604 - val_mae: 11.4597\n",
      "Epoch 37/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 219.4024 - mae: 11.3956 - val_loss: 222.1793 - val_mae: 11.4615\n",
      "Epoch 38/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.3699 - mae: 11.3942 - val_loss: 222.2557 - val_mae: 11.4636\n",
      "Epoch 39/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.3364 - mae: 11.3930 - val_loss: 222.2815 - val_mae: 11.4642\n",
      "Epoch 40/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 219.3142 - mae: 11.3922 - val_loss: 222.3152 - val_mae: 11.4636\n",
      "Epoch 41/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.2868 - mae: 11.3915 - val_loss: 222.3572 - val_mae: 11.4630\n",
      "Epoch 42/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.2705 - mae: 11.3909 - val_loss: 222.3865 - val_mae: 11.4639\n",
      "Epoch 43/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.2524 - mae: 11.3902 - val_loss: 222.4427 - val_mae: 11.4682\n",
      "Epoch 44/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.2318 - mae: 11.3896 - val_loss: 222.4720 - val_mae: 11.4689\n",
      "Epoch 45/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.2123 - mae: 11.3889 - val_loss: 222.5046 - val_mae: 11.4685\n",
      "Epoch 46/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.1920 - mae: 11.3883 - val_loss: 222.5124 - val_mae: 11.4691\n",
      "Epoch 47/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.1749 - mae: 11.3875 - val_loss: 222.5488 - val_mae: 11.4696\n",
      "Epoch 48/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.1508 - mae: 11.3870 - val_loss: 222.5810 - val_mae: 11.4703\n",
      "Epoch 49/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.1359 - mae: 11.3865 - val_loss: 222.6204 - val_mae: 11.4706\n",
      "Epoch 50/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.1153 - mae: 11.3858 - val_loss: 222.6459 - val_mae: 11.4724\n",
      "Epoch 51/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.0988 - mae: 11.3853 - val_loss: 222.6897 - val_mae: 11.4748\n",
      "Epoch 52/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 219.0863 - mae: 11.3848 - val_loss: 222.7176 - val_mae: 11.4762\n",
      "Epoch 53/100\n",
      "24901/24901 [==============================] - 52s 2ms/step - loss: 219.0642 - mae: 11.3840 - val_loss: 222.7630 - val_mae: 11.4755\n",
      "Epoch 54/100\n",
      "24901/24901 [==============================] - 53s 2ms/step - loss: 219.0537 - mae: 11.3835 - val_loss: 222.8168 - val_mae: 11.4784\n",
      "Epoch 55/100\n",
      "24901/24901 [==============================] - 54s 2ms/step - loss: 219.0411 - mae: 11.3831 - val_loss: 222.8158 - val_mae: 11.4787\n",
      "Epoch 56/100\n",
      "24901/24901 [==============================] - 55s 2ms/step - loss: 219.0278 - mae: 11.3827 - val_loss: 222.8511 - val_mae: 11.4795\n",
      "Epoch 57/100\n",
      "24901/24901 [==============================] - 55s 2ms/step - loss: 219.0117 - mae: 11.3821 - val_loss: 222.9040 - val_mae: 11.4804\n",
      "Epoch 58/100\n",
      "24901/24901 [==============================] - 54s 2ms/step - loss: 219.0022 - mae: 11.3816 - val_loss: 222.9473 - val_mae: 11.4821\n",
      "Epoch 59/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 218.9919 - mae: 11.3811 - val_loss: 222.9830 - val_mae: 11.4817\n",
      "Epoch 60/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.9754 - mae: 11.3806 - val_loss: 223.0044 - val_mae: 11.4865\n",
      "Epoch 61/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.9641 - mae: 11.3802 - val_loss: 223.0384 - val_mae: 11.4871\n",
      "Epoch 62/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.9480 - mae: 11.3797 - val_loss: 223.1079 - val_mae: 11.4897\n",
      "Epoch 63/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 218.9350 - mae: 11.3791 - val_loss: 223.1370 - val_mae: 11.4902\n",
      "Epoch 64/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.9207 - mae: 11.3788 - val_loss: 223.1365 - val_mae: 11.4938\n",
      "Epoch 65/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.8990 - mae: 11.3779 - val_loss: 223.2328 - val_mae: 11.4948\n",
      "Epoch 66/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.8767 - mae: 11.3769 - val_loss: 223.2495 - val_mae: 11.4984\n",
      "Epoch 67/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.8689 - mae: 11.3766 - val_loss: 223.2875 - val_mae: 11.4986\n",
      "Epoch 68/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.8608 - mae: 11.3762 - val_loss: 223.3390 - val_mae: 11.4987\n",
      "Epoch 69/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 218.8445 - mae: 11.3756 - val_loss: 223.3927 - val_mae: 11.5012\n",
      "Epoch 70/100\n",
      "24901/24901 [==============================] - 51s 2ms/step - loss: 218.8364 - mae: 11.3754 - val_loss: 223.4630 - val_mae: 11.5014\n",
      "Epoch 71/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.8250 - mae: 11.3750 - val_loss: 223.4811 - val_mae: 11.5020\n",
      "Epoch 72/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.8139 - mae: 11.3745 - val_loss: 223.5312 - val_mae: 11.5027\n",
      "Epoch 73/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.8009 - mae: 11.3740 - val_loss: 223.5643 - val_mae: 11.5047\n",
      "Epoch 74/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.7915 - mae: 11.3736 - val_loss: 223.5668 - val_mae: 11.5044\n",
      "Epoch 75/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.7962 - mae: 11.3736 - val_loss: 223.6311 - val_mae: 11.5056\n",
      "Epoch 76/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.7823 - mae: 11.3731 - val_loss: 223.6660 - val_mae: 11.5064\n",
      "Epoch 77/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.7701 - mae: 11.3728 - val_loss: 223.6921 - val_mae: 11.5077\n",
      "Epoch 78/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.7603 - mae: 11.3724 - val_loss: 223.7243 - val_mae: 11.5080\n",
      "Epoch 79/100\n",
      "24901/24901 [==============================] - 50s 2ms/step - loss: 218.7579 - mae: 11.3722 - val_loss: 223.7506 - val_mae: 11.5078\n",
      "Epoch 80/100\n",
      "24898/24901 [============================>.] - ETA: 0s - loss: 218.7452 - mae: 11.3718"
     ]
    }
   ],
   "source": [
    "data['Gender_encoded'] = pd.factorize(data['Gender'])[0]\n",
    "data['Occupation_encoded'] = pd.factorize(data['Occupation'])[0]\n",
    "\n",
    "features = ['Gender_encoded', 'Occupation_encoded', 'Birth year', 'Associated Countries']\n",
    "target = 'Age of death'\n",
    "model, history = run_neural_network(data, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using implementation #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using implementation #3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
